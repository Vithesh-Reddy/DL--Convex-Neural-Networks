{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OaPbkZbKX6pj"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "import random\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "jkl8RfxBYJX2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device available now:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTPLjHZwYJ48",
        "outputId": "1cba678b-6324-4ef0-b480-7bfd2221a6b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available now: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_transforms = transforms.Compose(\n",
        "         [transforms.ToTensor(),\n",
        "         transforms.CenterCrop(28),\n",
        "         transforms.Normalize((0.5), (0.5))]\n",
        ")"
      ],
      "metadata": {
        "id": "O7Gv0zrVYLAk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchsize = 32\n",
        "rowsize = 28\n",
        "colsize = 28\n",
        "inchannels = 3\n",
        "fc_size = 128\n",
        "no_classes = 2\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "df1nJfcTYNEE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\")\n",
        "cols = np.arange(0,58,1)\n",
        "df = pd.read_csv(download_url,names=cols)\n",
        "pd.set_option(\"display.max.rows\", None)\n",
        "data = pd.DataFrame(df).to_numpy()\n",
        "num = len(data)\n",
        "feat = len(data[0])-1"
      ],
      "metadata": {
        "id": "G2LBaGxDeLWB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[:,0:feat]\n",
        "label = data[:,-1]\n",
        "Xcopy = X\n",
        "labelcopy = label\n",
        "print(label)\n",
        "for i in range(int(len(label))):\n",
        "  label[i] = random.choice([0,1])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.2, random_state=42) \n",
        "X_train = torch.Tensor(X_train)\n",
        "X_test = torch.Tensor(X_test)\n",
        "y_train = torch.Tensor(y_train)\n",
        "y_test = torch.Tensor(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaJVEQfiisz4",
        "outputId": "660909f3-f81f-4615-ff6d-1179e21a53f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. ... 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rXTqUQMi1FH",
        "outputId": "3b347352-b3c5-4858-bdd0-94a51232befa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IOC_MLP(nn.Module):\n",
        "  def __init__(self,rowsize,colsize,fc_size,inchannels,no_classes):\n",
        "    super(IOC_MLP, self).__init__()\n",
        "    self.fc1 = nn.Linear(feat,fc_size)\n",
        "    self.batchnorm1 = nn.BatchNorm1d(fc_size)\n",
        "    self.fc2 = nn.Linear(fc_size,fc_size)\n",
        "    self.batchnorm2 = nn.BatchNorm1d(fc_size)\n",
        "    self.fc3 = nn.Linear(fc_size,fc_size)\n",
        "    self.batchnorm3 = nn.BatchNorm1d(fc_size)\n",
        "    self.fc4 = nn.Linear(fc_size,no_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x)\n",
        "    x = x.view(-1,feat)\n",
        "    x = F.elu(self.fc1(x))\n",
        "    x = self.batchnorm1(x)\n",
        "    x = F.elu(self.fc2(x))\n",
        "    x = self.batchnorm2(x)\n",
        "    x = F.elu(self.fc3(x))\n",
        "    x = self.batchnorm3(x)\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "b0AX6rXqf15B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self,rowsize,colsize,fc_size,inchannels,no_classes):\n",
        "    super(NN, self).__init__()\n",
        "    self.fc1 = nn.Linear(feat,fc_size)\n",
        "    self.batchnorm1 = nn.BatchNorm1d(fc_size)\n",
        "    self.fc2 = nn.Linear(fc_size,fc_size)\n",
        "    self.batchnorm2 = nn.BatchNorm1d(fc_size)\n",
        "    self.fc3 = nn.Linear(fc_size,fc_size)\n",
        "    self.batchnorm3 = nn.BatchNorm1d(fc_size)\n",
        "    self.fc4 = nn.Linear(fc_size,no_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x)\n",
        "    x = x.view(-1,feat)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.batchnorm1(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.batchnorm2(x)\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.batchnorm3(x)\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Pbjd_tuNRzuJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ioc_model = IOC_MLP(rowsize,colsize,fc_size,inchannels,no_classes).to(device)\n",
        "nn_model = NN(rowsize,colsize,fc_size,inchannels,no_classes).to(device)\n",
        "# model = SimpleAlex(no_classes).to(device)"
      ],
      "metadata": {
        "id": "f8LvMOUfhI_D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ioc_optimizer = torch.optim.Adam(ioc_model.parameters(),lr= 0.0001)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "nn_optimizer = torch.optim.Adam(nn_model.parameters(),lr= 0.0001)"
      ],
      "metadata": {
        "id": "8Ggb8UTJgrKx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.zeros(2)\n",
        "b = torch.zeros(2)\n",
        "a[0]=1\n",
        "b[1]=1\n",
        "m = {0:a,1:b}"
      ],
      "metadata": {
        "id": "xbcBTAoSoUaW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ioc = []\n",
        "train_nn = []\n",
        "test_ioc = 0\n",
        "test_ioc = 0"
      ],
      "metadata": {
        "id": "dL605XjDQ-y2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def training(model,traindata,testdata)\n",
        "# print(\"Choose the weight Variation number, (1 for clipping negative weight to zero , 2 for taking absolute of weights , 3 for exponentiation of weights\")\n",
        "# a = input()\n",
        "for i in range(epochs):\n",
        "  Loss = 0\n",
        "  num_correct=0\n",
        "  tot = 0\n",
        "  count=0\n",
        "  for d in range(int(len(X_train)/batchsize)):\n",
        "    # iterating through every batch\n",
        "    ls = np.arange(count,count+batchsize,1)\n",
        "    ls = torch.from_numpy(ls)\n",
        "    x_data = X_train[ls].to(device)\n",
        "    labels = y_train[ls].to(device)\n",
        "    ioc_optimizer.zero_grad()\n",
        "    outputs = ioc_model(x_data).to(device)\n",
        "    _, pred = torch.max(outputs.data, 1)\n",
        "    num_correct += (pred == labels).sum().item()\n",
        "    tot += labels.size(0)\n",
        "    onehot_labels = torch.zeros((batchsize,2))\n",
        "    for i in range(batchsize):\n",
        "      onehot_labels[i] = m[int(labels[i])]                                         \n",
        "    onehot_labels = onehot_labels.to(device)\n",
        "    loss = criterion(outputs, onehot_labels)\n",
        "    loss.backward()\n",
        "    ioc_optimizer.step()\n",
        "    Loss += loss\n",
        "    count+=batchsize\n",
        "    for w in ioc_model.parameters():\n",
        "      p = w\n",
        "      torch.where(w > 0, w, torch.exp(p))\n",
        "  train_ioc.append(num_correct/tot)\n",
        "  print(\"Train Accuracy for exponentiation of negative weights: \",num_correct/tot)\n",
        "  print(\"Loss: \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E25oWHigv52",
        "outputId": "cf506586-a07c-4ef7-c3a1-094d8b6e1445"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy for exponentiation of negative weights:  0.5122282608695652\n",
            "Loss:  0.7243381142616272\n",
            "Train Accuracy for exponentiation of negative weights:  0.5230978260869565\n",
            "Loss:  0.7082328796386719\n",
            "Train Accuracy for exponentiation of negative weights:  0.5388586956521739\n",
            "Loss:  0.694148600101471\n",
            "Train Accuracy for exponentiation of negative weights:  0.5521739130434783\n",
            "Loss:  0.681691586971283\n",
            "Train Accuracy for exponentiation of negative weights:  0.5584239130434783\n",
            "Loss:  0.67063307762146\n",
            "Train Accuracy for exponentiation of negative weights:  0.5652173913043478\n",
            "Loss:  0.6621412038803101\n",
            "Train Accuracy for exponentiation of negative weights:  0.5698369565217392\n",
            "Loss:  0.6544000506401062\n",
            "Train Accuracy for exponentiation of negative weights:  0.5728260869565217\n",
            "Loss:  0.6470270156860352\n",
            "Train Accuracy for exponentiation of negative weights:  0.5790760869565217\n",
            "Loss:  0.6397922039031982\n",
            "Train Accuracy for exponentiation of negative weights:  0.5826086956521739\n",
            "Loss:  0.6332944631576538\n",
            "Train Accuracy for exponentiation of negative weights:  0.5858695652173913\n",
            "Loss:  0.6280633807182312\n",
            "Train Accuracy for exponentiation of negative weights:  0.592391304347826\n",
            "Loss:  0.6235544681549072\n",
            "Train Accuracy for exponentiation of negative weights:  0.5948369565217392\n",
            "Loss:  0.6195272207260132\n",
            "Train Accuracy for exponentiation of negative weights:  0.5970108695652174\n",
            "Loss:  0.6153783202171326\n",
            "Train Accuracy for exponentiation of negative weights:  0.6\n",
            "Loss:  0.6125379800796509\n",
            "Train Accuracy for exponentiation of negative weights:  0.6035326086956522\n",
            "Loss:  0.6096551418304443\n",
            "Train Accuracy for exponentiation of negative weights:  0.6078804347826087\n",
            "Loss:  0.6064612865447998\n",
            "Train Accuracy for exponentiation of negative weights:  0.6141304347826086\n",
            "Loss:  0.6030522584915161\n",
            "Train Accuracy for exponentiation of negative weights:  0.6198369565217391\n",
            "Loss:  0.5985491275787354\n",
            "Train Accuracy for exponentiation of negative weights:  0.6255434782608695\n",
            "Loss:  0.5948994159698486\n",
            "Train Accuracy for exponentiation of negative weights:  0.6285326086956522\n",
            "Loss:  0.5909894704818726\n",
            "Train Accuracy for exponentiation of negative weights:  0.6326086956521739\n",
            "Loss:  0.5866310596466064\n",
            "Train Accuracy for exponentiation of negative weights:  0.6421195652173913\n",
            "Loss:  0.5825849175453186\n",
            "Train Accuracy for exponentiation of negative weights:  0.6467391304347826\n",
            "Loss:  0.5782541036605835\n",
            "Train Accuracy for exponentiation of negative weights:  0.653804347826087\n",
            "Loss:  0.573915958404541\n",
            "Train Accuracy for exponentiation of negative weights:  0.6608695652173913\n",
            "Loss:  0.5700604319572449\n",
            "Train Accuracy for exponentiation of negative weights:  0.6625\n",
            "Loss:  0.5654146671295166\n",
            "Train Accuracy for exponentiation of negative weights:  0.6679347826086957\n",
            "Loss:  0.5616769194602966\n",
            "Train Accuracy for exponentiation of negative weights:  0.6698369565217391\n",
            "Loss:  0.5576193332672119\n",
            "Train Accuracy for exponentiation of negative weights:  0.6779891304347826\n",
            "Loss:  0.5529664754867554\n",
            "Train Accuracy for exponentiation of negative weights:  0.6790760869565218\n",
            "Loss:  0.5493258833885193\n",
            "Train Accuracy for exponentiation of negative weights:  0.6842391304347826\n",
            "Loss:  0.545384407043457\n",
            "Train Accuracy for exponentiation of negative weights:  0.688858695652174\n",
            "Loss:  0.5425149202346802\n",
            "Train Accuracy for exponentiation of negative weights:  0.69375\n",
            "Loss:  0.5392314195632935\n",
            "Train Accuracy for exponentiation of negative weights:  0.6975543478260869\n",
            "Loss:  0.5361783504486084\n",
            "Train Accuracy for exponentiation of negative weights:  0.6997282608695652\n",
            "Loss:  0.5328966975212097\n",
            "Train Accuracy for exponentiation of negative weights:  0.7035326086956522\n",
            "Loss:  0.5296931266784668\n",
            "Train Accuracy for exponentiation of negative weights:  0.7067934782608696\n",
            "Loss:  0.5268349051475525\n",
            "Train Accuracy for exponentiation of negative weights:  0.7119565217391305\n",
            "Loss:  0.5235124230384827\n",
            "Train Accuracy for exponentiation of negative weights:  0.7133152173913043\n",
            "Loss:  0.5209407806396484\n",
            "Train Accuracy for exponentiation of negative weights:  0.720108695652174\n",
            "Loss:  0.5174461603164673\n",
            "Train Accuracy for exponentiation of negative weights:  0.7233695652173913\n",
            "Loss:  0.5139085054397583\n",
            "Train Accuracy for exponentiation of negative weights:  0.7269021739130435\n",
            "Loss:  0.5100877285003662\n",
            "Train Accuracy for exponentiation of negative weights:  0.7307065217391304\n",
            "Loss:  0.5078218579292297\n",
            "Train Accuracy for exponentiation of negative weights:  0.7377717391304348\n",
            "Loss:  0.5032736659049988\n",
            "Train Accuracy for exponentiation of negative weights:  0.7423913043478261\n",
            "Loss:  0.5025181770324707\n",
            "Train Accuracy for exponentiation of negative weights:  0.7429347826086956\n",
            "Loss:  0.4983935058116913\n",
            "Train Accuracy for exponentiation of negative weights:  0.747554347826087\n",
            "Loss:  0.495449960231781\n",
            "Train Accuracy for exponentiation of negative weights:  0.7510869565217392\n",
            "Loss:  0.49161431193351746\n",
            "Train Accuracy for exponentiation of negative weights:  0.7535326086956522\n",
            "Loss:  0.49009883403778076\n",
            "Train Accuracy for exponentiation of negative weights:  0.7567934782608695\n",
            "Loss:  0.4853370189666748\n",
            "Train Accuracy for exponentiation of negative weights:  0.7592391304347826\n",
            "Loss:  0.4839942455291748\n",
            "Train Accuracy for exponentiation of negative weights:  0.7605978260869565\n",
            "Loss:  0.47970137000083923\n",
            "Train Accuracy for exponentiation of negative weights:  0.7660326086956522\n",
            "Loss:  0.4755084216594696\n",
            "Train Accuracy for exponentiation of negative weights:  0.7695652173913043\n",
            "Loss:  0.47139498591423035\n",
            "Train Accuracy for exponentiation of negative weights:  0.7706521739130435\n",
            "Loss:  0.4669685363769531\n",
            "Train Accuracy for exponentiation of negative weights:  0.7736413043478261\n",
            "Loss:  0.4625515341758728\n",
            "Train Accuracy for exponentiation of negative weights:  0.7752717391304348\n",
            "Loss:  0.46004027128219604\n",
            "Train Accuracy for exponentiation of negative weights:  0.7771739130434783\n",
            "Loss:  0.4569244384765625\n",
            "Train Accuracy for exponentiation of negative weights:  0.7823369565217392\n",
            "Loss:  0.45643216371536255\n",
            "Train Accuracy for exponentiation of negative weights:  0.7847826086956522\n",
            "Loss:  0.4538409113883972\n",
            "Train Accuracy for exponentiation of negative weights:  0.7858695652173913\n",
            "Loss:  0.45674094557762146\n",
            "Train Accuracy for exponentiation of negative weights:  0.7855978260869565\n",
            "Loss:  0.45370692014694214\n",
            "Train Accuracy for exponentiation of negative weights:  0.7921195652173914\n",
            "Loss:  0.4569067060947418\n",
            "Train Accuracy for exponentiation of negative weights:  0.7923913043478261\n",
            "Loss:  0.45501893758773804\n",
            "Train Accuracy for exponentiation of negative weights:  0.7948369565217391\n",
            "Loss:  0.46469253301620483\n",
            "Train Accuracy for exponentiation of negative weights:  0.7951086956521739\n",
            "Loss:  0.4524078369140625\n",
            "Train Accuracy for exponentiation of negative weights:  0.7972826086956522\n",
            "Loss:  0.4676740765571594\n",
            "Train Accuracy for exponentiation of negative weights:  0.79375\n",
            "Loss:  0.4487767815589905\n",
            "Train Accuracy for exponentiation of negative weights:  0.7888586956521739\n",
            "Loss:  0.4642176628112793\n",
            "Train Accuracy for exponentiation of negative weights:  0.7804347826086957\n",
            "Loss:  0.44263532757759094\n",
            "Train Accuracy for exponentiation of negative weights:  0.7505434782608695\n",
            "Loss:  0.6115584373474121\n",
            "Train Accuracy for exponentiation of negative weights:  0.7464673913043478\n",
            "Loss:  0.5343396067619324\n",
            "Train Accuracy for exponentiation of negative weights:  0.752445652173913\n",
            "Loss:  0.44077011942863464\n",
            "Train Accuracy for exponentiation of negative weights:  0.7673913043478261\n",
            "Loss:  0.4316646456718445\n",
            "Train Accuracy for exponentiation of negative weights:  0.7730978260869565\n",
            "Loss:  0.43934038281440735\n",
            "Train Accuracy for exponentiation of negative weights:  0.782608695652174\n",
            "Loss:  0.4433898329734802\n",
            "Train Accuracy for exponentiation of negative weights:  0.7845108695652174\n",
            "Loss:  0.4455159604549408\n",
            "Train Accuracy for exponentiation of negative weights:  0.7875\n",
            "Loss:  0.44716718792915344\n",
            "Train Accuracy for exponentiation of negative weights:  0.7948369565217391\n",
            "Loss:  0.44323673844337463\n",
            "Train Accuracy for exponentiation of negative weights:  0.8002717391304348\n",
            "Loss:  0.435978502035141\n",
            "Train Accuracy for exponentiation of negative weights:  0.8029891304347826\n",
            "Loss:  0.4283680319786072\n",
            "Train Accuracy for exponentiation of negative weights:  0.8065217391304348\n",
            "Loss:  0.42189672589302063\n",
            "Train Accuracy for exponentiation of negative weights:  0.8097826086956522\n",
            "Loss:  0.41534316539764404\n",
            "Train Accuracy for exponentiation of negative weights:  0.8135869565217392\n",
            "Loss:  0.4094340205192566\n",
            "Train Accuracy for exponentiation of negative weights:  0.8160326086956522\n",
            "Loss:  0.40381062030792236\n",
            "Train Accuracy for exponentiation of negative weights:  0.8214673913043479\n",
            "Loss:  0.3992989957332611\n",
            "Train Accuracy for exponentiation of negative weights:  0.8271739130434783\n",
            "Loss:  0.3941802978515625\n",
            "Train Accuracy for exponentiation of negative weights:  0.8296195652173913\n",
            "Loss:  0.3895445466041565\n",
            "Train Accuracy for exponentiation of negative weights:  0.8336956521739131\n",
            "Loss:  0.3840552568435669\n",
            "Train Accuracy for exponentiation of negative weights:  0.8364130434782608\n",
            "Loss:  0.3794914186000824\n",
            "Train Accuracy for exponentiation of negative weights:  0.8388586956521739\n",
            "Loss:  0.37516719102859497\n",
            "Train Accuracy for exponentiation of negative weights:  0.8421195652173913\n",
            "Loss:  0.36969566345214844\n",
            "Train Accuracy for exponentiation of negative weights:  0.841304347826087\n",
            "Loss:  0.3683915436267853\n",
            "Train Accuracy for exponentiation of negative weights:  0.8288043478260869\n",
            "Loss:  0.44719210267066956\n",
            "Train Accuracy for exponentiation of negative weights:  0.8078804347826087\n",
            "Loss:  0.37266385555267334\n",
            "Train Accuracy for exponentiation of negative weights:  0.8141304347826087\n",
            "Loss:  0.4574429392814636\n",
            "Train Accuracy for exponentiation of negative weights:  0.825\n",
            "Loss:  0.4330819845199585\n",
            "Train Accuracy for exponentiation of negative weights:  0.8309782608695652\n",
            "Loss:  0.39312905073165894\n",
            "Train Accuracy for exponentiation of negative weights:  0.83125\n",
            "Loss:  0.3636575937271118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    count=0\n",
        "    for d in range(int(len(X_test)/batchsize)):\n",
        "        ls = np.arange(count,count+batchsize,1)\n",
        "        ls = torch.from_numpy(ls)\n",
        "        x_data = X_test[ls].to(device)\n",
        "        labels = y_test[ls].to(device)\n",
        "        outputs = ioc_model(x_data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        count+=batchsize\n",
        "    test_ioc = 100*correct/total\n",
        "    print(\"Test Accuracy on exponentiation of negative weights For IOC Model : \",100 * correct / total)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zmgt6nz1G-G",
        "outputId": "61a6dfbe-30ac-4fb6-fbb1-c908d302fd4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy on exponentiation of negative weights For IOC Model :  50.669642857142854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def training(model,traindata,testdata)\n",
        "# print(\"Choose the weight Variation number, (1 for clipping negative weight to zero , 2 for taking absolute of weights , 3 for exponentiation of weights\")\n",
        "# a = input()\n",
        "for i in range(epochs):\n",
        "  Loss = 0\n",
        "  num_correct=0\n",
        "  tot = 0\n",
        "  count=0\n",
        "  for d in range(int(len(X_train)/batchsize)):\n",
        "    # iterating through every batch\n",
        "    ls = np.arange(count,count+batchsize,1)\n",
        "    ls = torch.from_numpy(ls)\n",
        "    x_data = X_train[ls].to(device)\n",
        "    labels = y_train[ls].to(device)\n",
        "    nn_optimizer.zero_grad()\n",
        "    outputs = nn_model(x_data).to(device)\n",
        "    _, pred = torch.max(outputs.data, 1)\n",
        "    num_correct += (pred == labels).sum().item()\n",
        "    tot += labels.size(0)\n",
        "    onehot_labels = torch.zeros((batchsize,2))\n",
        "    for i in range(batchsize):\n",
        "      onehot_labels[i] = m[int(labels[i])]                                         \n",
        "    onehot_labels = onehot_labels.to(device)\n",
        "    loss = criterion(outputs, onehot_labels)\n",
        "    loss.backward()\n",
        "    nn_optimizer.step()\n",
        "    Loss += loss\n",
        "    count+=batchsize\n",
        "  train_nn.append(num_correct/tot)\n",
        "  \n",
        "  print(\"Train Accuracy for NN: \",num_correct/tot)\n",
        "  print(\"Loss: \",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdAiAfwhTR_J",
        "outputId": "a46c05b7-8ad7-4f74-998f-96da0a0f38da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy for NN:  0.5046195652173913\n",
            "Loss:  0.7459298372268677\n",
            "Train Accuracy for NN:  0.5364130434782609\n",
            "Loss:  0.6989479064941406\n",
            "Train Accuracy for NN:  0.5733695652173914\n",
            "Loss:  0.6748161315917969\n",
            "Train Accuracy for NN:  0.5904891304347826\n",
            "Loss:  0.6611651182174683\n",
            "Train Accuracy for NN:  0.6073369565217391\n",
            "Loss:  0.6537641286849976\n",
            "Train Accuracy for NN:  0.6176630434782608\n",
            "Loss:  0.6469166874885559\n",
            "Train Accuracy for NN:  0.6301630434782609\n",
            "Loss:  0.62863689661026\n",
            "Train Accuracy for NN:  0.6467391304347826\n",
            "Loss:  0.6141177415847778\n",
            "Train Accuracy for NN:  0.6616847826086957\n",
            "Loss:  0.6161836385726929\n",
            "Train Accuracy for NN:  0.6660326086956522\n",
            "Loss:  0.612356424331665\n",
            "Train Accuracy for NN:  0.6817934782608696\n",
            "Loss:  0.5982841849327087\n",
            "Train Accuracy for NN:  0.6902173913043478\n",
            "Loss:  0.5855699777603149\n",
            "Train Accuracy for NN:  0.7002717391304348\n",
            "Loss:  0.5854202508926392\n",
            "Train Accuracy for NN:  0.70625\n",
            "Loss:  0.5653283596038818\n",
            "Train Accuracy for NN:  0.7146739130434783\n",
            "Loss:  0.5623708963394165\n",
            "Train Accuracy for NN:  0.7228260869565217\n",
            "Loss:  0.5416733622550964\n",
            "Train Accuracy for NN:  0.73125\n",
            "Loss:  0.5444546341896057\n",
            "Train Accuracy for NN:  0.7388586956521739\n",
            "Loss:  0.5661740303039551\n",
            "Train Accuracy for NN:  0.747554347826087\n",
            "Loss:  0.5383331775665283\n",
            "Train Accuracy for NN:  0.7453804347826087\n",
            "Loss:  0.5259212851524353\n",
            "Train Accuracy for NN:  0.7529891304347827\n",
            "Loss:  0.5139995813369751\n",
            "Train Accuracy for NN:  0.7638586956521739\n",
            "Loss:  0.5259110927581787\n",
            "Train Accuracy for NN:  0.772554347826087\n",
            "Loss:  0.49843209981918335\n",
            "Train Accuracy for NN:  0.7828804347826087\n",
            "Loss:  0.5021896362304688\n",
            "Train Accuracy for NN:  0.7875\n",
            "Loss:  0.4750833511352539\n",
            "Train Accuracy for NN:  0.7948369565217391\n",
            "Loss:  0.4873881936073303\n",
            "Train Accuracy for NN:  0.7970108695652174\n",
            "Loss:  0.48453277349472046\n",
            "Train Accuracy for NN:  0.8008152173913043\n",
            "Loss:  0.4671945869922638\n",
            "Train Accuracy for NN:  0.814945652173913\n",
            "Loss:  0.4458739161491394\n",
            "Train Accuracy for NN:  0.8144021739130435\n",
            "Loss:  0.44933146238327026\n",
            "Train Accuracy for NN:  0.8263586956521739\n",
            "Loss:  0.4226189851760864\n",
            "Train Accuracy for NN:  0.8266304347826087\n",
            "Loss:  0.42426180839538574\n",
            "Train Accuracy for NN:  0.8380434782608696\n",
            "Loss:  0.42423126101493835\n",
            "Train Accuracy for NN:  0.8375\n",
            "Loss:  0.405409574508667\n",
            "Train Accuracy for NN:  0.8410326086956522\n",
            "Loss:  0.3975622057914734\n",
            "Train Accuracy for NN:  0.8502717391304347\n",
            "Loss:  0.35358408093452454\n",
            "Train Accuracy for NN:  0.8470108695652174\n",
            "Loss:  0.3688989579677582\n",
            "Train Accuracy for NN:  0.8535326086956522\n",
            "Loss:  0.42428699135780334\n",
            "Train Accuracy for NN:  0.8521739130434782\n",
            "Loss:  0.37374982237815857\n",
            "Train Accuracy for NN:  0.8519021739130435\n",
            "Loss:  0.3580302894115448\n",
            "Train Accuracy for NN:  0.8497282608695652\n",
            "Loss:  0.31697869300842285\n",
            "Train Accuracy for NN:  0.8671195652173913\n",
            "Loss:  0.33841216564178467\n",
            "Train Accuracy for NN:  0.8717391304347826\n",
            "Loss:  0.37545502185821533\n",
            "Train Accuracy for NN:  0.876358695652174\n",
            "Loss:  0.3316793143749237\n",
            "Train Accuracy for NN:  0.8807065217391304\n",
            "Loss:  0.2867124080657959\n",
            "Train Accuracy for NN:  0.8777173913043478\n",
            "Loss:  0.2955969274044037\n",
            "Train Accuracy for NN:  0.8845108695652174\n",
            "Loss:  0.2551743984222412\n",
            "Train Accuracy for NN:  0.8777173913043478\n",
            "Loss:  0.3612504005432129\n",
            "Train Accuracy for NN:  0.8823369565217392\n",
            "Loss:  0.28620007634162903\n",
            "Train Accuracy for NN:  0.8896739130434783\n",
            "Loss:  0.2423723042011261\n",
            "Train Accuracy for NN:  0.8790760869565217\n",
            "Loss:  0.24362322688102722\n",
            "Train Accuracy for NN:  0.8932065217391304\n",
            "Loss:  0.23872463405132294\n",
            "Train Accuracy for NN:  0.9016304347826087\n",
            "Loss:  0.2502988874912262\n",
            "Train Accuracy for NN:  0.8972826086956521\n",
            "Loss:  0.25864124298095703\n",
            "Train Accuracy for NN:  0.8899456521739131\n",
            "Loss:  0.22586211562156677\n",
            "Train Accuracy for NN:  0.8888586956521739\n",
            "Loss:  0.23267757892608643\n",
            "Train Accuracy for NN:  0.8720108695652173\n",
            "Loss:  0.29458582401275635\n",
            "Train Accuracy for NN:  0.8744565217391305\n",
            "Loss:  0.25680863857269287\n",
            "Train Accuracy for NN:  0.8769021739130435\n",
            "Loss:  0.2468901425600052\n",
            "Train Accuracy for NN:  0.8853260869565217\n",
            "Loss:  0.38846802711486816\n",
            "Train Accuracy for NN:  0.8858695652173914\n",
            "Loss:  0.2919135093688965\n",
            "Train Accuracy for NN:  0.9029891304347826\n",
            "Loss:  0.32974958419799805\n",
            "Train Accuracy for NN:  0.9122282608695652\n",
            "Loss:  0.37963148951530457\n",
            "Train Accuracy for NN:  0.9220108695652174\n",
            "Loss:  0.30138152837753296\n",
            "Train Accuracy for NN:  0.9296195652173913\n",
            "Loss:  0.22596459090709686\n",
            "Train Accuracy for NN:  0.935054347826087\n",
            "Loss:  0.22216416895389557\n",
            "Train Accuracy for NN:  0.9334239130434783\n",
            "Loss:  0.19655409455299377\n",
            "Train Accuracy for NN:  0.941304347826087\n",
            "Loss:  0.2000783383846283\n",
            "Train Accuracy for NN:  0.9377717391304348\n",
            "Loss:  0.1851004958152771\n",
            "Train Accuracy for NN:  0.9358695652173913\n",
            "Loss:  0.1696511059999466\n",
            "Train Accuracy for NN:  0.9480978260869565\n",
            "Loss:  0.1573682725429535\n",
            "Train Accuracy for NN:  0.9394021739130435\n",
            "Loss:  0.21271881461143494\n",
            "Train Accuracy for NN:  0.9315217391304348\n",
            "Loss:  0.1997825652360916\n",
            "Train Accuracy for NN:  0.9377717391304348\n",
            "Loss:  0.156685471534729\n",
            "Train Accuracy for NN:  0.939945652173913\n",
            "Loss:  0.17432716488838196\n",
            "Train Accuracy for NN:  0.9445652173913044\n",
            "Loss:  0.1547718346118927\n",
            "Train Accuracy for NN:  0.9391304347826087\n",
            "Loss:  0.1960565447807312\n",
            "Train Accuracy for NN:  0.9394021739130435\n",
            "Loss:  0.15500915050506592\n",
            "Train Accuracy for NN:  0.9426630434782609\n",
            "Loss:  0.14780163764953613\n",
            "Train Accuracy for NN:  0.939945652173913\n",
            "Loss:  0.13855639100074768\n",
            "Train Accuracy for NN:  0.939945652173913\n",
            "Loss:  0.12426462769508362\n",
            "Train Accuracy for NN:  0.9470108695652174\n",
            "Loss:  0.12591257691383362\n",
            "Train Accuracy for NN:  0.9334239130434783\n",
            "Loss:  0.18094393610954285\n",
            "Train Accuracy for NN:  0.9383152173913043\n",
            "Loss:  0.13848315179347992\n",
            "Train Accuracy for NN:  0.9345108695652173\n",
            "Loss:  0.15056651830673218\n",
            "Train Accuracy for NN:  0.9260869565217391\n",
            "Loss:  0.13305479288101196\n",
            "Train Accuracy for NN:  0.9323369565217391\n",
            "Loss:  0.13794884085655212\n",
            "Train Accuracy for NN:  0.9334239130434783\n",
            "Loss:  0.1853700876235962\n",
            "Train Accuracy for NN:  0.9271739130434783\n",
            "Loss:  0.14441047608852386\n",
            "Train Accuracy for NN:  0.9255434782608696\n",
            "Loss:  0.1698533147573471\n",
            "Train Accuracy for NN:  0.9323369565217391\n",
            "Loss:  0.13306233286857605\n",
            "Train Accuracy for NN:  0.9391304347826087\n",
            "Loss:  0.12245220690965652\n",
            "Train Accuracy for NN:  0.9380434782608695\n",
            "Loss:  0.14173024892807007\n",
            "Train Accuracy for NN:  0.9464673913043479\n",
            "Loss:  0.11307702213525772\n",
            "Train Accuracy for NN:  0.9513586956521739\n",
            "Loss:  0.10883095860481262\n",
            "Train Accuracy for NN:  0.964945652173913\n",
            "Loss:  0.10182775557041168\n",
            "Train Accuracy for NN:  0.9722826086956522\n",
            "Loss:  0.0868878960609436\n",
            "Train Accuracy for NN:  0.9788043478260869\n",
            "Loss:  0.07776821404695511\n",
            "Train Accuracy for NN:  0.9793478260869565\n",
            "Loss:  0.07837013155221939\n",
            "Train Accuracy for NN:  0.9834239130434783\n",
            "Loss:  0.07293779402971268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    count=0\n",
        "    for d in range(int(len(X_test)/batchsize)):\n",
        "        ls = np.arange(count,count+batchsize,1)\n",
        "        ls = torch.from_numpy(ls)\n",
        "        x_data = X_test[ls].to(device)\n",
        "        labels = y_test[ls].to(device)\n",
        "        outputs = nn_model(x_data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        count+=batchsize\n",
        "    test_nn = 100*correct/total\n",
        "    print(\"Test Accuracy For NN Model : \",100 * correct / total)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJtD0Lb6Tc4j",
        "outputId": "cc0ae2af-7631-412d-e2cb-94b7557aafd3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy For NN Model :  52.00892857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epoch_numbers = np.arange(0,epochs,1)\n",
        "plt.title(\"Training Accuracies\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracies\")\n",
        "plt.plot(epoch_numbers,train_ioc,label='IOC Model')\n",
        "plt.plot(epoch_numbers,train_nn,label='NN Model')     \n",
        "plt.legend()\n",
        "plt.show()           "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-nA6QgfIXgn9",
        "outputId": "941eef01-e3b4-48ef-c2b3-c05dddad0e54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generalization Gap for NN Model\",(train_nn[len(train_nn)-1]*100)-test_nn)\n",
        "print(\"Generalization Gap for IOC Model\",(train_ioc[len(train_ioc)-1]*100)-test_ioc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDV1RJfGa6sa",
        "outputId": "d3883a64-119a-4e64-c6cf-c4e10db18c37"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generalization Gap for NN Model 46.33346273291926\n",
            "Generalization Gap for IOC Model 32.455357142857146\n"
          ]
        }
      ]
    }
  ]
}